{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_columns',372)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in train.columns:\n",
    "    if train[i].sum() == 0:\n",
    "        del train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x  = train.drop(['TARGET'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train.TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = train_x.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "log_model = linear_model.LogisticRegression()\n",
    "scores = cross_validation.cross_val_score(log_model, train_x, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.960431465882\n"
     ]
    }
   ],
   "source": [
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = list(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. var38 (0.404620)\n",
      "2. var15 (0.156771)\n",
      "3. saldo_medio_var5_hace3 (0.025348)\n",
      "4. saldo_medio_var5_ult3 (0.024600)\n",
      "5. num_var45_ult3 (0.020675)\n",
      "6. num_var45_hace3 (0.019134)\n",
      "7. num_var45_hace2 (0.016148)\n",
      "8. num_var22_ult3 (0.013596)\n",
      "9. saldo_medio_var5_hace2 (0.013336)\n",
      "10. num_var45_ult1 (0.012835)\n",
      "11. num_med_var45_ult3 (0.012167)\n",
      "12. saldo_var42 (0.010974)\n",
      "13. saldo_var30 (0.010926)\n",
      "14. saldo_medio_var5_ult1 (0.010884)\n",
      "15. num_var22_hace3 (0.010763)\n",
      "16. num_var22_hace2 (0.010677)\n",
      "17. num_meses_var5_ult3 (0.010428)\n",
      "18. saldo_var5 (0.009937)\n",
      "19. var36 (0.008470)\n",
      "20. num_var22_ult1 (0.008096)\n",
      "21. num_meses_var39_vig_ult3 (0.008011)\n",
      "22. num_med_var22_ult3 (0.006457)\n",
      "23. ind_var30 (0.006308)\n",
      "24. ind_var5 (0.005168)\n",
      "25. imp_op_var41_efect_ult3 (0.003816)\n",
      "26. imp_op_var39_efect_ult3 (0.003765)\n",
      "27. imp_op_var41_comer_ult3 (0.003693)\n",
      "28. imp_op_var39_ult1 (0.003682)\n",
      "29. imp_op_var41_ult1 (0.003650)\n",
      "30. imp_op_var39_comer_ult3 (0.003526)\n",
      "31. imp_ent_var16_ult1 (0.003457)\n",
      "32. var3 (0.003448)\n",
      "33. num_ent_var16_ult1 (0.003433)\n",
      "34. imp_trans_var37_ult1 (0.003417)\n",
      "35. num_op_var41_ult3 (0.003357)\n",
      "36. num_op_var39_ult3 (0.003336)\n",
      "37. num_op_var39_comer_ult3 (0.003229)\n",
      "38. num_op_var41_hace2 (0.003203)\n",
      "39. imp_op_var39_comer_ult1 (0.003183)\n",
      "40. num_op_var41_comer_ult3 (0.003142)\n",
      "41. imp_op_var41_comer_ult1 (0.003134)\n",
      "42. num_op_var39_hace2 (0.003116)\n",
      "43. num_op_var39_efect_ult3 (0.003094)\n",
      "44. imp_op_var39_efect_ult1 (0.003071)\n",
      "45. imp_op_var41_efect_ult1 (0.003064)\n",
      "46. num_op_var41_ult1 (0.003044)\n",
      "47. num_op_var41_efect_ult3 (0.002997)\n",
      "48. num_op_var39_ult1 (0.002991)\n",
      "49. num_var42 (0.002974)\n",
      "50. num_var43_recib_ult1 (0.002832)\n",
      "51. num_op_var41_comer_ult1 (0.002749)\n",
      "52. num_op_var39_comer_ult1 (0.002743)\n",
      "53. imp_var43_emit_ult1 (0.002494)\n",
      "54. saldo_var37 (0.002479)\n",
      "55. num_var43_emit_ult1 (0.002451)\n",
      "56. ind_var43_recib_ult1 (0.002338)\n",
      "57. num_op_var41_efect_ult1 (0.002309)\n",
      "58. num_op_var39_efect_ult1 (0.002303)\n",
      "59. num_var4 (0.002298)\n",
      "60. num_var35 (0.002247)\n",
      "61. num_var30 (0.002098)\n",
      "62. ind_var43_emit_ult1 (0.001998)\n",
      "63. num_var37_med_ult2 (0.001873)\n",
      "64. num_var41_0 (0.001618)\n",
      "65. num_var37 (0.001470)\n",
      "66. num_var37_0 (0.001454)\n",
      "67. num_var5 (0.001429)\n",
      "68. num_var39_0 (0.001399)\n",
      "69. num_trasp_var11_ult1 (0.001097)\n",
      "70. saldo_medio_var8_ult3 (0.000995)\n",
      "71. num_op_var41_hace3 (0.000951)\n",
      "72. num_op_var39_hace3 (0.000938)\n",
      "73. saldo_medio_var8_ult1 (0.000931)\n",
      "74. ind_var8_0 (0.000876)\n",
      "75. num_meses_var8_ult3 (0.000870)\n",
      "76. saldo_var8 (0.000841)\n",
      "77. saldo_medio_var8_hace2 (0.000808)\n",
      "78. num_var30_0 (0.000769)\n",
      "79. num_var8_0 (0.000744)\n",
      "80. saldo_var26 (0.000736)\n",
      "81. ind_var37_cte (0.000727)\n",
      "82. num_var42_0 (0.000717)\n",
      "83. saldo_var25 (0.000680)\n",
      "84. num_var5_0 (0.000626)\n",
      "85. ind_var41_0 (0.000625)\n",
      "86. ind_var39_0 (0.000603)\n",
      "87. ind_var37_0 (0.000594)\n",
      "88. ind_var37 (0.000584)\n",
      "89. ind_var5_0 (0.000578)\n",
      "90. ind_var12_0 (0.000521)\n",
      "91. num_sal_var16_ult1 (0.000518)\n",
      "92. ind_var9_cte_ult1 (0.000500)\n",
      "93. ind_var9_ult1 (0.000480)\n",
      "94. ind_var10cte_ult1 (0.000474)\n",
      "95. ind_var10_ult1 (0.000463)\n",
      "96. num_var12_0 (0.000438)\n",
      "97. ind_var26_cte (0.000434)\n",
      "98. num_var26 (0.000428)\n",
      "99. saldo_medio_var12_ult1 (0.000423)\n",
      "100. saldo_medio_var8_hace3 (0.000423)\n",
      "101. num_var25 (0.000418)\n",
      "102. num_meses_var12_ult3 (0.000410)\n",
      "103. imp_sal_var16_ult1 (0.000402)\n",
      "104. saldo_medio_var12_ult3 (0.000391)\n",
      "105. saldo_var13 (0.000381)\n",
      "106. num_var26_0 (0.000376)\n",
      "107. num_var25_0 (0.000368)\n",
      "108. ind_var25_cte (0.000365)\n",
      "109. saldo_medio_var12_hace2 (0.000364)\n",
      "110. saldo_medio_var13_corto_ult3 (0.000360)\n",
      "111. saldo_var12 (0.000351)\n",
      "112. saldo_medio_var13_corto_ult1 (0.000350)\n",
      "113. imp_var7_recib_ult1 (0.000346)\n",
      "114. saldo_medio_var13_corto_hace2 (0.000344)\n",
      "115. ind_var13 (0.000343)\n",
      "116. saldo_var13_corto (0.000342)\n",
      "117. ind_var8 (0.000340)\n",
      "118. num_var1_0 (0.000333)\n",
      "119. ind_var7_recib_ult1 (0.000332)\n",
      "120. ind_var24_0 (0.000332)\n",
      "121. num_var40_0 (0.000328)\n",
      "122. imp_op_var40_efect_ult1 (0.000320)\n",
      "123. ind_var13_0 (0.000312)\n",
      "124. ind_var12 (0.000311)\n",
      "125. num_var7_recib_ult1 (0.000301)\n",
      "126. saldo_var24 (0.000284)\n",
      "127. saldo_medio_var12_hace3 (0.000282)\n",
      "128. var21 (0.000280)\n",
      "129. num_var24_0 (0.000270)\n",
      "130. num_var8 (0.000263)\n",
      "131. num_var14_0 (0.000261)\n",
      "132. ind_var26 (0.000258)\n",
      "133. imp_op_var40_efect_ult3 (0.000257)\n",
      "134. ind_var40_0 (0.000253)\n",
      "135. ind_var26_0 (0.000245)\n",
      "136. ind_var1_0 (0.000242)\n",
      "137. ind_var32_cte (0.000241)\n",
      "138. ind_var24 (0.000237)\n",
      "139. ind_var14_0 (0.000232)\n",
      "140. ind_var30_0 (0.000230)\n",
      "141. ind_var25_0 (0.000219)\n",
      "142. num_op_var40_efect_ult3 (0.000217)\n",
      "143. ind_var25 (0.000213)\n",
      "144. num_var12 (0.000204)\n",
      "145. num_op_var40_efect_ult1 (0.000185)\n",
      "146. ind_var19 (0.000170)\n",
      "147. ind_var13_corto (0.000161)\n",
      "148. num_var13_0 (0.000154)\n",
      "149. ind_var13_corto_0 (0.000152)\n",
      "150. num_var24 (0.000151)\n",
      "151. saldo_var40 (0.000150)\n",
      "152. saldo_var1 (0.000138)\n",
      "153. num_meses_var13_corto_ult3 (0.000132)\n",
      "154. imp_op_var40_comer_ult3 (0.000128)\n",
      "155. num_op_var40_comer_ult3 (0.000125)\n",
      "156. num_var13_corto (0.000120)\n",
      "157. imp_op_var40_comer_ult1 (0.000115)\n",
      "158. imp_op_var40_ult1 (0.000111)\n",
      "159. num_op_var40_ult3 (0.000105)\n",
      "160. num_var13_corto_0 (0.000100)\n",
      "161. num_var13 (0.000091)\n",
      "162. num_op_var40_comer_ult1 (0.000091)\n",
      "163. num_op_var40_ult1 (0.000090)\n",
      "164. ind_var39 (0.000090)\n",
      "165. num_var39 (0.000089)\n",
      "166. saldo_medio_var13_corto_hace3 (0.000087)\n",
      "167. ind_var40 (0.000087)\n",
      "168. imp_aport_var13_hace3 (0.000083)\n",
      "169. num_reemb_var17_ult1 (0.000082)\n",
      "170. ind_var1 (0.000076)\n",
      "171. saldo_var14 (0.000075)\n",
      "172. num_var40 (0.000074)\n",
      "173. ind_var14 (0.000070)\n",
      "174. delta_num_aport_var13_1y3 (0.000069)\n",
      "175. num_var14 (0.000068)\n",
      "176. ind_var31_0 (0.000065)\n",
      "177. num_var1 (0.000064)\n",
      "178. ind_var20_0 (0.000064)\n",
      "179. delta_imp_aport_var13_1y3 (0.000063)\n",
      "180. imp_aport_var13_ult1 (0.000063)\n",
      "181. num_var20_0 (0.000062)\n",
      "182. saldo_var32 (0.000058)\n",
      "183. imp_reemb_var17_ult1 (0.000057)\n",
      "184. num_var32 (0.000055)\n",
      "185. ind_var32_0 (0.000055)\n",
      "186. imp_compra_var44_ult1 (0.000054)\n",
      "187. num_op_var40_hace2 (0.000050)\n",
      "188. ind_var32 (0.000050)\n",
      "189. ind_var13_largo (0.000048)\n",
      "190. ind_var13_largo_0 (0.000046)\n",
      "191. num_var31_0 (0.000045)\n",
      "192. saldo_var31 (0.000044)\n",
      "193. num_var32_0 (0.000043)\n",
      "194. num_aport_var13_hace3 (0.000043)\n",
      "195. ind_var31 (0.000038)\n",
      "196. num_aport_var17_ult1 (0.000036)\n",
      "197. num_var31 (0.000036)\n",
      "198. saldo_var44 (0.000035)\n",
      "199. num_var17_0 (0.000033)\n",
      "200. num_var17 (0.000032)\n",
      "201. saldo_medio_var17_ult1 (0.000031)\n",
      "202. ind_var17_0 (0.000029)\n",
      "203. delta_num_compra_var44_1y3 (0.000028)\n",
      "204. saldo_medio_var17_ult3 (0.000027)\n",
      "205. saldo_var17 (0.000027)\n",
      "206. saldo_medio_var44_ult3 (0.000027)\n",
      "207. imp_reemb_var13_ult1 (0.000027)\n",
      "208. num_var13_largo_0 (0.000027)\n",
      "209. num_meses_var17_ult3 (0.000025)\n",
      "210. num_var13_largo (0.000025)\n",
      "211. saldo_var13_largo (0.000024)\n",
      "212. imp_aport_var17_ult1 (0.000023)\n",
      "213. delta_imp_compra_var44_1y3 (0.000023)\n",
      "214. num_meses_var44_ult3 (0.000023)\n",
      "215. ind_var17 (0.000023)\n",
      "216. num_meses_var13_largo_ult3 (0.000023)\n",
      "217. saldo_medio_var44_ult1 (0.000022)\n",
      "218. ind_var44 (0.000019)\n",
      "219. num_var20 (0.000017)\n",
      "220. num_aport_var13_ult1 (0.000016)\n",
      "221. delta_num_reemb_var13_1y3 (0.000016)\n",
      "222. num_var44_0 (0.000013)\n",
      "223. ind_var44_0 (0.000012)\n",
      "224. delta_imp_reemb_var13_1y3 (0.000011)\n",
      "225. num_var44 (0.000010)\n",
      "226. ind_var33_0 (0.000010)\n",
      "227. num_compra_var44_ult1 (0.000010)\n",
      "228. num_reemb_var13_ult1 (0.000009)\n",
      "229. saldo_medio_var13_largo_ult1 (0.000008)\n",
      "230. saldo_medio_var13_largo_hace2 (0.000008)\n",
      "231. ind_var20 (0.000007)\n",
      "232. saldo_medio_var13_largo_ult3 (0.000006)\n",
      "233. saldo_medio_var44_hace2 (0.000006)\n",
      "234. num_venta_var44_ult1 (0.000006)\n",
      "235. delta_imp_reemb_var17_1y3 (0.000006)\n",
      "236. delta_imp_venta_var44_1y3 (0.000006)\n",
      "237. delta_num_aport_var17_1y3 (0.000005)\n",
      "238. imp_venta_var44_ult1 (0.000005)\n",
      "239. imp_compra_var44_hace3 (0.000005)\n",
      "240. num_var33_0 (0.000005)\n",
      "241. saldo_medio_var17_hace2 (0.000005)\n",
      "242. delta_num_reemb_var17_1y3 (0.000005)\n",
      "243. ind_var33 (0.000004)\n",
      "244. num_var33 (0.000004)\n",
      "245. delta_num_venta_var44_1y3 (0.000004)\n",
      "246. delta_imp_aport_var17_1y3 (0.000004)\n",
      "247. num_venta_var44_hace3 (0.000003)\n",
      "248. saldo_var33 (0.000003)\n",
      "249. saldo_medio_var44_hace3 (0.000002)\n",
      "250. delta_imp_aport_var33_1y3 (0.000002)\n",
      "251. num_meses_var33_ult3 (0.000002)\n",
      "252. saldo_medio_var13_largo_hace3 (0.000002)\n",
      "253. saldo_medio_var33_ult3 (0.000002)\n",
      "254. imp_trasp_var17_in_ult1 (0.000002)\n",
      "255. num_var29_0 (0.000001)\n",
      "256. saldo_medio_var33_hace2 (0.000001)\n",
      "257. num_meses_var29_ult3 (0.000001)\n",
      "258. num_var7_emit_ult1 (0.000001)\n",
      "259. ind_var7_emit_ult1 (0.000001)\n",
      "260. num_var6_0 (0.000001)\n",
      "261. saldo_medio_var33_ult1 (0.000001)\n",
      "262. num_compra_var44_hace3 (0.000001)\n",
      "263. imp_aport_var33_ult1 (0.000001)\n",
      "264. delta_num_trasp_var33_in_1y3 (0.000001)\n",
      "265. ind_var13_medio (0.000001)\n",
      "266. num_var18_0 (0.000001)\n",
      "267. saldo_var20 (0.000001)\n",
      "268. ind_var18_0 (0.000001)\n",
      "269. delta_imp_amort_var34_1y3 (0.000001)\n",
      "270. saldo_medio_var17_hace3 (0.000001)\n",
      "271. imp_aport_var33_hace3 (0.000001)\n",
      "272. imp_var7_emit_ult1 (0.000001)\n",
      "273. ind_var6_0 (0.000001)\n",
      "274. delta_num_trasp_var17_in_1y3 (0.000000)\n",
      "275. saldo_medio_var29_hace2 (0.000000)\n",
      "276. saldo_medio_var29_ult1 (0.000000)\n",
      "277. ind_var29 (0.000000)\n",
      "278. delta_num_aport_var33_1y3 (0.000000)\n",
      "279. num_trasp_var33_in_hace3 (0.000000)\n",
      "280. saldo_var29 (0.000000)\n",
      "281. saldo_var6 (0.000000)\n",
      "282. ind_var34_0 (0.000000)\n",
      "283. ind_var29_0 (0.000000)\n",
      "284. num_aport_var33_hace3 (0.000000)\n",
      "285. imp_trasp_var17_out_ult1 (0.000000)\n",
      "286. delta_imp_trasp_var17_in_1y3 (0.000000)\n",
      "287. num_aport_var17_hace3 (0.000000)\n",
      "288. delta_imp_trasp_var33_in_1y3 (0.000000)\n",
      "289. num_var6 (0.000000)\n",
      "290. delta_imp_trasp_var17_out_1y3 (0.000000)\n",
      "291. imp_venta_var44_hace3 (0.000000)\n",
      "292. num_var34_0 (0.000000)\n",
      "293. saldo_medio_var13_medio_hace2 (0.000000)\n",
      "294. saldo_medio_var13_medio_ult3 (0.000000)\n",
      "295. num_trasp_var17_in_ult1 (0.000000)\n",
      "296. num_trasp_var33_in_ult1 (0.000000)\n",
      "297. saldo_medio_var13_medio_ult1 (0.000000)\n",
      "298. saldo_var18 (0.000000)\n",
      "299. delta_num_trasp_var17_out_1y3 (0.000000)\n",
      "300. saldo_medio_var29_hace3 (0.000000)\n",
      "301. num_trasp_var17_out_ult1 (0.000000)\n",
      "302. saldo_var34 (0.000000)\n",
      "303. ind_var34 (0.000000)\n",
      "304. imp_trasp_var33_in_ult1 (0.000000)\n",
      "305. num_var18 (0.000000)\n",
      "306. imp_aport_var17_hace3 (0.000000)\n",
      "307. saldo_medio_var33_hace3 (0.000000)\n",
      "308. num_aport_var33_ult1 (0.000000)\n",
      "309. delta_imp_amort_var18_1y3 (0.000000)\n",
      "310. saldo_medio_var29_ult3 (0.000000)\n",
      "311. num_op_var40_hace3 (0.000000)\n",
      "312. num_reemb_var17_hace3 (0.000000)\n",
      "313. imp_reemb_var17_hace3 (0.000000)\n",
      "314. num_var29 (0.000000)\n",
      "315. ind_var18 (0.000000)\n",
      "316. ind_var6 (0.000000)\n",
      "317. num_var13_medio_0 (0.000000)\n",
      "318. num_var13_medio (0.000000)\n",
      "319. saldo_var13_medio (0.000000)\n",
      "320. imp_amort_var34_ult1 (0.000000)\n",
      "321. imp_trasp_var33_in_hace3 (0.000000)\n",
      "322. delta_imp_trasp_var33_out_1y3 (0.000000)\n",
      "323. num_meses_var13_medio_ult3 (0.000000)\n",
      "324. delta_num_reemb_var33_1y3 (0.000000)\n",
      "325. num_reemb_var33_ult1 (0.000000)\n",
      "326. num_trasp_var17_in_hace3 (0.000000)\n",
      "327. num_var34 (0.000000)\n",
      "328. num_trasp_var33_out_ult1 (0.000000)\n",
      "329. delta_imp_reemb_var33_1y3 (0.000000)\n",
      "330. delta_num_trasp_var33_out_1y3 (0.000000)\n",
      "331. imp_amort_var18_ult1 (0.000000)\n",
      "332. ind_var13_medio_0 (0.000000)\n",
      "333. imp_reemb_var33_ult1 (0.000000)\n",
      "334. imp_trasp_var17_in_hace3 (0.000000)\n",
      "335. imp_trasp_var33_out_ult1 (0.000000)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-13928adcee26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%d. %s (%f)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimportances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Plot the feature importances of the forest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Feature importances\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m plt.bar(range(X.shape[1]), importances[indices],\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "forest = ExtraTreesClassifier(n_estimators=250, random_state=0)\n",
    "X=train_x\n",
    "X=X.dropna()\n",
    "y=train_ytrain_x\n",
    "forest.fit(X,y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f+1, c[indices[f]], importances[indices[f]]))\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(1, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(forest, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "random_search.fit(train_x, train_y)\n",
    "# scores = cross_validation.cross_val_score(forest, train_x, train_y, cv=3)\n",
    "# print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # In[3]:\n",
    "\n",
    "# def print_full(x):\n",
    "#     pd.set_option('display.max_rows', len(x))\n",
    "#     print(x)\n",
    "#     pd.reset_option('display.max_rows')\n",
    "\n",
    "\n",
    "# # In[4]:\n",
    "\n",
    "# colu = pd.read_excel('Data Dictionary.xls',sheetname=0)\n",
    "# listcol = list(colu.Name)\n",
    "\n",
    "\n",
    "# date = [\"_DT\",\"_TS\",\"_TM\"]\n",
    "\n",
    "\n",
    "# # In[8]:\n",
    "\n",
    "# tate = []\n",
    "# for i in listcol:\n",
    "#     if (i[-3:] in date):\n",
    "#         tate.append(i)\n",
    "\n",
    "\n",
    "# # In[9]:\n",
    "\n",
    "# for col in tate:\n",
    "#         del datas[col]\n",
    "\n",
    "\n",
    "# # In[10]:\n",
    "\n",
    "# cate=[]\n",
    "# s = [\"_CD\",\"_ID\",\"IND\",\"OME\"]\n",
    "# for i in listcol:\n",
    "#     if (i[-3:] in s) & (i!='AUTH_ID'):\n",
    "#         cate.append(i)\n",
    "\n",
    "# # In[12]:\n",
    "\n",
    "# for dfgs in cate:\n",
    "#     affiliate_channel_maxs = []\n",
    "#     affiliate_channel_maxs_dict = {}\n",
    "#     affiliate_channel_maxs = list(enumerate(np.unique(datas[dfgs])))\n",
    "#     affiliate_channel_maxs_dict = { name : i for i, name in affiliate_channel_maxs }\n",
    "#     datas[dfgs] = datas[dfgs].map(lambda x: affiliate_channel_maxs_dict[x]).astype(int)\n",
    "\n",
    "\n",
    "# # In[13]:\n",
    "\n",
    "# datas = datas.drop(['ACCT_ID_TOKEN'], axis=1)\n",
    "\n",
    "\n",
    "# # In[14]:\n",
    "\n",
    "# datas = datas.drop(['AUTH_ID'], axis=1)\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "# backcoh = ['AUTHZN_TRMNL_PIN_CAPBLT_NUM']\n",
    "# for dfgs in backcoh:\n",
    "#     affiliate_channel_maxs = []\n",
    "#     affiliate_channel_maxs_dict = {}\n",
    "#     affiliate_channel_maxs = list(enumerate(np.unique(datas[dfgs])))\n",
    "#     affiliate_channel_maxs_dict = { name : i for i, name in affiliate_channel_maxs }\n",
    "#     datas[dfgs] = datas[dfgs].map(lambda x: affiliate_channel_maxs_dict[x]).astype(int)\n",
    "\n",
    "\n",
    "# # In[16]:\n",
    "\n",
    "# for i in datas.columns:\n",
    "#     datas[i] = datas[i].astype(int)\n",
    "\n",
    "\n",
    "# # In[17]:\n",
    "\n",
    "# datas.info(verbose=True, null_counts=True)\n",
    "\n",
    "\n",
    "# # In[18]:\n",
    "\n",
    "# train_y = datas['FRD_IND']\n",
    "# train_x = datas.drop(['FRD_IND','Unnamed: 0','Unnamed: 0.1'], axis=1)\n",
    "\n",
    "\n",
    "# # In[ ]:\n",
    "\n",
    "# from sklearn import cross_validation\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from scipy.stats import randint as sp_randint\n",
    "# from sklearn.grid_search import RandomizedSearchCV\n",
    "# forest = RandomForestClassifier(n_estimators=100)\n",
    "# param_dist = {\"max_depth\": [3, None],\n",
    "#               \"max_features\": sp_randint(1, 11),\n",
    "#               \"min_samples_split\": sp_randint(1, 11),\n",
    "#               \"min_samples_leaf\": sp_randint(1, 11),\n",
    "#               \"bootstrap\": [True, False],\n",
    "#               \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# # run randomized search\n",
    "# n_iter_search = 20\n",
    "# random_search = RandomizedSearchCV(forest, param_distributions=param_dist,\n",
    "#                                    n_iter=n_iter_search)\n",
    "# random_search.fit(train_x, train_y)\n",
    "# # scores = cross_validation.cross_val_score(forest, train_x, train_y, cv=3)\n",
    "# # print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import operator\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "columns_name = [\"timestamp\",\"placement_id\",\"browser_id\",\"os_id\",\"region\",\"country\",\"is_adserver\",\"campaign\",\n",
    "                \"creative_asset_id\",\"mouseovers\",\"clicks\",\"max_duration\",\"video_length\",\"viewable\"]\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "df_train = pd.read_table('code-test0000_part_00',sep=\"|\",names=columns_name)\n",
    "# df_test = pd.read_table(test,sep=\"|\",names=columns_name,header=None)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "df_train\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "df_train.info()\n",
    "# df_test.info()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "df_train.drop([\"is_adserver\",\"max_duration\",\"video_length\"],axis=1,inplace=True)\n",
    "# df_test.drop([\"is_adserver\",\"max_duration\",\"video_length\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "df_train\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "df_train = df_train.fillna('-1')\n",
    "# df_test = df_test.fillna('-1')\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "df_train.info()\n",
    "# df_test.info()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "# Y_train = df_train([])\n",
    "# X_train = df_train.drop([\"clic\"])\n",
    "mask=df_train[\"country\"]==\"US\"\n",
    "np.unique(df_train[mask].region)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "tfa = np.vstack(df_train.timestamp.astype(str).apply(lambda x: list(map(str, [x[11:13],x[14:16],x[17:19]]))))\n",
    "df_train['hours'] = tfa[:,0]\n",
    "df_train['minute'] = tfa[:,1]\n",
    "df_train['seconds'] = tfa[:,2]\n",
    "df_train = df_train.drop(['timestamp'], axis=1)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "df_train = df_train.drop(['timestamp'], axis=1)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "com = list(df_train.columns)\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "df_train.info()\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "d = ['clicks','hours','minute','seconds']\n",
    "for i in d:\n",
    "    com.remove(i)\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "df_train = df_train[1:5000]\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "sum = 0\n",
    "for f in com:\n",
    "    dfg = np.unique(list(df_train[f].values))\n",
    "    print len(dfg), f\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "for i in df_train.columns:\n",
    "    if (i != 'placement_id') and (i != 'creative_asset_id') and (i != 'browser_id'):\n",
    "        print pd.DataFrame(pd.value_counts(df_train[i]))\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "for f in com:\n",
    "    df_all_dummy = pd.get_dummies(df_train[f], prefix=f)\n",
    "    df_train = df_train.drop([f], axis=1)\n",
    "    df_train = pd.concat((df_train, df_all_dummy), axis=1)\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "for dfgs in com:\n",
    "    affiliate_channel_maxs = []\n",
    "    affiliate_channel_maxs_dict = {}\n",
    "    affiliate_channel_maxs = list(enumerate(np.unique(df_train[dfgs])))\n",
    "    affiliate_channel_maxs_dict = { name : i for i, name in affiliate_channel_maxs }\n",
    "    df_train[dfgs] = df_train[dfgs].map(lambda x: affiliate_channel_maxs_dict[x]).astype(int)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "df_train\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "for i in df_train.columns:\n",
    "    df_train[i] = df_train[i].astype(int)\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "df_train.info()\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "# #Greedy add 1 by 1\n",
    "# scr = []\n",
    "# c = list(df_train.columns)\n",
    "# c.remove('clicks')\n",
    "# for t in range(800):\n",
    "#     score={}\n",
    "#     for f in c:\n",
    "#         if f in scr:\n",
    "#             pass\n",
    "#         else:\n",
    "#             scr.append(f)\n",
    "#             scr.append('clicks')\n",
    "#             xz=df_train[scr]\n",
    "#             xz = xz.dropna()\n",
    "#             y_t=xz['clicks']\n",
    "# #             print y_t.shape\n",
    "#             if len(y_t.index) == 0:\n",
    "#                 scr.remove(f)\n",
    "#                 scr.remove('clicks')\n",
    "#                 continue\n",
    "#             x_t=xz.drop(['clicks'],axis=1)\n",
    "# #             print x_t.shape\n",
    "#             log_model = linear_model.LogisticRegression(penalty='l1',solver='liblinear',multi_class='ovr')\n",
    "#             #log_model.fit(pd.DataFrame(xz[scr]), pd.DataFrame(xz['Response']))\n",
    "#             log_model.fit(x_t,y_t)\n",
    "#             score[f]=log_model.score(x_t,y_t)\n",
    "#             #score.append([f,log_model.score(pd.DataFrame(xz[scr]),pd.DataFrame(xz['Response']))])\n",
    "#             scr.remove(f)\n",
    "#             scr.remove('clicks')\n",
    "# #     sorted_score = sorted(score.items(), key=operator.itemgetter(1))\n",
    "# #     print sorted_score\n",
    "#     scr.append(max(score.iteritems(), key=operator.itemgetter(1))[0])\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "# scr\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "train_y = df_train['clicks']\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "train_x = df_train.drop(['clicks'], axis=1)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "# from xgboost.sklearn import XGBClassifier\n",
    "# xgg = XGBClassifier(max_depth=6, learning_rate=0.3, n_estimators=25,\n",
    "#                     objective='binary:logistic', subsample=0.5, colsample_bytree=0.5, seed=0)\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100, min_samples_split=10, random_state=415)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(forest, train_x, train_y, cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for f in train.columns:\n",
    "    dfg = np.unique(list(train[f].values))\n",
    "    #sum = sum +len(dfg)\n",
    "    print len(dfg), f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in train.columns:\n",
    "    if (i != 'id') and (i != 'date_account_created'):\n",
    "        print pd.DataFrame(pd.value_counts(train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for data in (train, test):\n",
    "    data['year_created']  = data['date_account_created'].apply(lambda x: 'nan' if str(x) == 'nan' else int(str(x)[:4]))\n",
    "    data['month_created'] = data['date_account_created'].apply(lambda x: 'nan' if str(x) == 'nan' else int(str(x)[5:7]))\n",
    "    data['week_created']  = data['date_account_created'].apply(lambda x: 'nan' if str(x) == 'nan' else int(str(x)[8:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['first_affiliate_tracked'].fillna('missing', inplace=True)\n",
    "av = train.age.values\n",
    "train['age'] = np.where(np.logical_or(av<14, av>88), 1, av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainVisitGroup = train.groupby(['country_destination'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregations = {\n",
    "    'month_created': {'month_created_max': 'max'},\n",
    "    'signup_flow': {'Countsignup': 'count' },\n",
    "    'language': {'Countlang': 'count'},\n",
    "    'affiliate_channel': {'affiliate_channel_max': 'max'},\n",
    "    'affiliate_provider': {'affiliate_provider_max': 'max'},\n",
    "    'first_affiliate_tracked': {'first_affiliate_tracked_max': 'max'},\n",
    "    'signup_app': {'signup_app_max': 'max'},\n",
    "    'first_device_type': {'first_device_type_max': 'max'},\n",
    "    'first_browser': {'first_browser_max': 'max'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "affiliate_channel_maxs = list(enumerate(np.unique(trainDF.affiliate_channel_max)))\n",
    "affiliate_channel_maxs_dict = { name : i for i, name in affiliate_channel_maxs }\n",
    "trainDF.affiliate_channel_max = trainDF.affiliate_channel_max.map(lambda x: affiliate_channel_maxs_dict[x]).astype(int)\n",
    "\n",
    "first_affiliate_tracked_maxs = list(enumerate(np.unique(trainDF.first_affiliate_tracked_max)))\n",
    "first_affiliate_tracked_maxs_dict = { name : i for i, name in first_affiliate_tracked_maxs }\n",
    "trainDF.first_affiliate_tracked_max = trainDF.first_affiliate_tracked_max.map(lambda x: first_affiliate_tracked_maxs_dict[x]).astype(int)\n",
    "\n",
    "affiliate_provider_maxs = list(enumerate(np.unique(trainDF.affiliate_provider_max)))\n",
    "affiliate_provider_max_dict = { name : i for i, name in affiliate_provider_maxs }\n",
    "trainDF.affiliate_provider_max = trainDF.affiliate_provider_max.map(lambda x: affiliate_provider_max_dict[x]).astype(int)\n",
    "\n",
    "first_device_type_maxs = list(enumerate(np.unique(trainDF.first_device_type_max)))\n",
    "first_device_type_maxs_dict = { name : i for i, name in first_device_type_maxs }\n",
    "trainDF.first_device_type_max = trainDF.first_device_type_max.map(lambda x: first_device_type_maxs_dict[x]).astype(int)\n",
    "\n",
    "signup_app_maxs = list(enumerate(np.unique(trainDF.signup_app_max)))\n",
    "signup_app_maxs_dict = { name : i for i, name in signup_app_maxs }\n",
    "trainDF.signup_app_max = trainDF.signup_app_max.map(lambda x: signup_app_maxs_dict[x]).astype(int)\n",
    "\n",
    "first_browser_maxs = list(enumerate(np.unique(trainDF.first_browser_max)))\n",
    "first_browser_maxs_dict = { name : i for i, name in first_browser_maxs }\n",
    "trainDF.first_browser_max = trainDF.first_browser_max.map(lambda x: first_browser_maxs_dict[x]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Greedy add 1 by 1\n",
    "scr = []\n",
    "c = [\"Product_Info_4\", \"Ins_Age\", \"Ht\", \"Wt\", \"BMI\", \"Employment_Info_1\", \"Employment_Info_4\", \"Employment_Info_6\",\n",
    "              \"Insurance_History_5\", \"Family_Hist_2\", \"Family_Hist_3\", \"Family_Hist_4\", \"Family_Hist_5\"]\n",
    "for t in range(1):\n",
    "    score={}\n",
    "    for f in c:\n",
    "        if f in scr:\n",
    "            pass\n",
    "        else:\n",
    "            scr.append(f)\n",
    "            scr.append('Response')\n",
    "            xz=train[scr]\n",
    "            xz = xz.dropna()\n",
    "            y_t=xz['Response']\n",
    "            x_t=xz.drop(['Response'],axis=1)\n",
    "            log_model = linear_model.LogisticRegression(solver='lbfgs',multi_class='multinomial')\n",
    "            #log_model.fit(pd.DataFrame(xz[scr]), pd.DataFrame(xz['Response']))\n",
    "            log_model.fit(x_t,y_t)\n",
    "            score[f]=log_model.score(x_t,y_t)\n",
    "            #score.append([f,log_model.score(pd.DataFrame(xz[scr]),pd.DataFrame(xz['Response']))])\n",
    "            scr.remove(f)\n",
    "            scr.remove('Response')\n",
    "    print score\n",
    "    scr.append(max(score.iteritems(), key=operator.itemgetter(1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = ExtraTreesClassifier(n_estimators=250, random_state=0)\n",
    "c = [\"Product_Info_4\", \"Ins_Age\", \"Ht\", \"Wt\", \"BMI\",\"Family_Hist_2\",\"Employment_Info_1\",\"Response\"]\n",
    "#\"Employment_Info_1\", \"Employment_Info_4\", \"Employment_Info_6\",\"Insurance_History_5\"\n",
    "X=train[c]\n",
    "X=X.dropna()\n",
    "y=X['Response']\n",
    "X=X.drop(['Response'],axis=1)\n",
    "X.info()\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f+1, c[indices[f]], importances[indices[f]]))\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def greedy(df,target, columns=None, params=50, gap=0.0000099):\n",
    "    train, test, y_t, y_test = cross_validation.train_test_split(df, df[target], test_size=0.2, random_state=0)\n",
    "    #train=df\n",
    "    scr = []\n",
    "    if columns is None:\n",
    "        columns=df.columns\n",
    "    diff=0.1\n",
    "    t=0\n",
    "    while t<=params:\n",
    "        score={}\n",
    "        for f in columns:\n",
    "            if f in scr:\n",
    "                pass\n",
    "            else:\n",
    "                if scr == []:\n",
    "                    past_score=0\n",
    "                else:\n",
    "                    past_score=temp\n",
    "                scr.append(f)\n",
    "                #print scr\n",
    "                #scr.append(target)\n",
    "                x_t = train[scr]\n",
    "                x_test=test[scr]\n",
    "                #x_t = x_t.dropna()\n",
    "                #x_test = x_test.dropna()\n",
    "                #y_t=x_t[target]\n",
    "                #print y_t.shape\n",
    "                if len(x_t.index) == 0:\n",
    "                    scr.remove(f)\n",
    "                    #scr.remove(target)\n",
    "                    continue\n",
    "                #x_t=x_t.drop([target],axis=1)\n",
    "                #model = linear_model.LogisticRegression(C=0.2, class_weight='balanced', random_state=12, n_jobs=-1)\n",
    "                #model.fit(x_t,y_t)\n",
    "                #model=RandomForestClassifier(n_estimators=300, criterion='gini', max_depth=10, min_samples_leaf=1, max_features=0.4, n_jobs=3, random_state=SEED) \n",
    "                #score[f]=cross_validation.cross_val_score(model,x_t,y_t,cv=5, scoring='roc_auc').mean()\n",
    "                model = XGBClassifier(learning_rate =0.05, n_estimators=100, max_depth=4, min_child_weight=10, \n",
    "                      gamma=0.2, reg_alpha=0.8, reg_lambda=0.8, #subsample=0.9, colsample_bytree=1,\n",
    "                      objective= 'binary:logistic', seed=12)\n",
    "                #score[f]=cross_validation.cross_val_score(model,x_t,y_t,cv=5, scoring='roc_auc').mean()\n",
    "                model.fit(x_t,y_t)\n",
    "                score[f]=model.score(x_test,y_test)\n",
    "                scr.remove(f)\n",
    "                #scr.remove(target)\n",
    "        sorted_score = sorted(score.items(), key=operator.itemgetter(1))\n",
    "        #print sorted_score\n",
    "        temp=max(score.iteritems(), key=operator.itemgetter(1))[1]\n",
    "        if temp-past_score < gap:\n",
    "            break\n",
    "        else:\n",
    "            added=max(score.iteritems(), key=operator.itemgetter(1))[0]\n",
    "            print \"HHHHH \\n\",added\n",
    "            scr.append(added)\n",
    "            print temp\n",
    "            t=t+1\n",
    "        print scr\n",
    "    return scr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
